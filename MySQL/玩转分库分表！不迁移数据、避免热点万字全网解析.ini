玩转分库分表！不迁移数据、避免热点万字全网解析


前言
    我们在谈论数据库架构和数据库优化的时候，我们'经常会听到“分库分表”、“分片”、“Sharding”…这样的关键词'。
    '公司业务量'正在（或者即将面临）'高速增长'，'技术方面'也'面临着一些挑战'。
    让人感到担忧的是，他们系统'真的就需要“分库分表”了'吗？

    “分库分表”有那么容易实践吗？
    为此，笔者整理了分库分表中可能遇到的一些问题，并结合以往经验介绍了对应的解决思路和建议。
    '中大型项目中'，'一旦遇到数据量比较大'，小伙伴应该'都知道就应该对数据进行拆分'了。


'有垂直和水平两种'。

    垂直拆分比较简单，也就是'本来一个数据库，数据量大之后，从业务角度进行拆分多个库'。
    如下图，独立的拆分出订单库和用户库。

        拆分前：
        DB_1
            订单表
            用户表

        拆分后：
        
        DB_1
            订单表
        DB_2
            用户表

    水平拆分的概念，是'同一个业务' '数据量大之后'，进行'水平拆分'。

        订单库1
            t.order表('4000万订单')

        拆分为

        订单库1
            t_order_1表('1000万订单')
            t_order_2表('1000万订单')
            t_order_3表('1000万订单')
            t_order_4表('1000万订单')

    上图中订单数据达到了4000万，我们也知道'mysql单表存储量' '推荐是百万级'，
    如果不进行处理，mysql'单表数据太大'，会'导致性能变慢'。

    使用方案可以参考'数据进行水平拆分'。把4000万数据拆分4张表或者更多。
    当然'也可以分库'，'再分表'；把压力从数据库层级分开。

分库分表方案

'分库分表方案'中有'常用的方案'，'hash取模'和'range范围'方案；
分库分表方案'最主要就是路由算法'，把路由的key按照指定的算法进行路由存放。

下边来介绍一下'两个方案的特点'。

1、hash取模方案

    在我们'设计系统之前'，可以'先预估一下' '大概这几年的订单量'，
    如：4000万。'每张表我们可以容纳1000万'，我们'可以设计4张表进行存储'。

    那具体如何路由存储的呢？'hash的方案'就是'拿指定的路由key（如：id）' 对'分表总数'进行'取模'，

    比如
        id=12的订单，对4进行取模，也就是会得到0，那此订单会放到0表中。
        id=13的订单，取模得到为1，就会放到1表中。
        
        '为什么对4取模'，是因为'分表总数是4'。
    
    优点：
        '订单数据'可以'均匀的放到' '那4张表中'，这样此订单进行操作时，就不会有'热点问题'。

        热点的含义：'对订单进行操作集中到1个表中'，'其他表的操作很少'。(操作热点)
        
        订单有个特点就是时间属性，一般用户操作订单数据，都会集中到'这段时间产生的订单'。
        如果'这段时间产生的订单' '都在同一张订单表中'，那就'会形成热点'，'那张表'的'压力会比较大'。
    缺点：
        将来的'数据迁移和扩容'，'会很难'。
        如：业务发展很好，订单量很大，超出了4000万的量，那我们就需要增加分表数。如果我们增加4个表

        一旦我们'增加了分表的总数'，'取模的基数'就会'变成8'，
        以前'id=12的订单' '按照此方案'就'会到4表中'查询，
        但之前的此订单时在0表的，这样就导致了数据查不到。

        就是因为'取模的基数' '产生了变化'。
        
        遇到这个情况，我们小伙伴'想到的方案'就是'做数据迁移'，把之前的4000万数据，重新做一个hash方案，放到新的规划分表中。
        也就是我们要做数据迁移。'这个是很痛苦的事情'。'有些小公司'可以'接受晚上停机迁移'，但'大公司是不允许停机做数据迁移的'。

        当然'做数据迁移' 可以'结合自己的公司的业务'，做一个工具进行，不过也带来了很多工作量，'每次扩容都要做数据迁移'
        那'有没有不需要做数据迁移的方案呢'，我们看下面的方案

2、range范围方案
    'range方案'也就是'以范围进行拆分数据'。

    range方案比较简单，就是'把一定范围内的订单，存放到一个表中'；
    如上图id=12放到0表中，id=1300万的放到1表中。
    
    '设计这个方案'时就是'前期把表的范围设计好'。'通过id进行路由存放'。

    优点
    我们小伙伴们想一下，'此方案'是不是有利于将来的扩容，'不需要做数据迁移'。'即使再增加4张表'，
    之前的4张表的范围不需要改变，id=12的还是在0表，id=1300万的还是在1表，
    '新增的4张表他们的范围'肯定是' 大于 4000万之后的范围'划分的。

    缺点
    有热点问题，我们想一下，因为'id的值会一直递增变大'，
    那'这段时间的订单'是不是'会一直在某一张表中'，
    
    如id=1000万 ～ id=2000万之间，'这段时间产生的订单'是不是都会'集中到此张表中'，这个就导致'有一个表过热，压力过大'，
    而其他的表没有什么压力。

3、总结：
    
    hash取模方案：'没有热点问题'，但'扩容迁移数据痛苦'

    range方案：'不需要迁移数据'，但'有热点问题'。

    

4.那'有什么方案可以做到两者的优点结合'呢？，'即不需要迁移数据，又能解决数据热点的问题呢？'

  
    其实还有一个现实需求，能否根据服务器的'性能以及存储高低'，'适当均匀调整存储'呢？

    方案思路

        hash是可以解决数据均匀的问题，range可以解决数据迁移问题，那我们可以不可以两者相结合呢？利用这两者的特性呢？

        我们考虑一下数据的扩容代表着，路由key（如id）的值变大了，
        这个是一定的，那我们先保证数据变大的时候，
        
        首先'用range方案'让数据落地到一个范围里面。
        这样'以后id再变大'，那以前的数据是不需要迁移的。

        但'又要考虑到数据均匀'，那是不是可以在一定的范围内数据均匀的呢？
        因为我们每次的扩容肯定会事'先设计好这次扩容的范围大小'，我们'只要保证这次的范围内的数据均匀'是不是就ok了。


    方案设计

    
        我们先定义一个group组概念，这组里面包含了一些分库以及分表，如下图

        Group01['id=0~4000万']
            |
            |如何定位DB (hash取模)
            |
            |
            --> DB_0 (id = 0~4000万)
                ｜
                ｜-->table_0 (id=0~1000万)
                ｜-->table_1 (id=1000万~2000万)
                ｜-->table_2 (id=2000万~3000万)
                ｜-->table_3 (id=3000万~4000万
                ｜-->
            --> DB_1 (id = 0~4000万)
                |
                ｜-->table_0 (id=0~1000万)
                ｜-->table_1 (id=1000万~'2500万')
                ｜-->table_2 (id=2500万~'4000万')

            --> DB_2 (id = 0~4000万)
                |
                ｜-->table_0 (id=0~1000万)
                ｜-->table_1 (id=1000万~'2500万')
                ｜-->table_2 (id=2500万~'4000万')



        上图有几个关键点：

        1）id=0～4000万'肯定落到group01组中'
        2）group01组有3个DB，那一个id如何'路由到哪个DB'？
        3）根据'hash取模定位DB'，那'模数为多少'？模数要为所有此group组DB中的表数，上图总表数为10。'为什么要用表的总数？'而不是DB总数3呢？
        4）如id=12，id%10=2；那值为2，落到哪个DB库呢？这是设计是前期设定好的，那怎么设定的呢？
        5）一旦设计定位哪个DB后，'就需要确定落到DB中的哪张表呢'？

        核心主流程

            id=15000
                步骤一：根据范围range，定位是哪个group组，(O < id < 4000万）
            group01
                步骤二：根据hash方案，定位是哪个DB (id % 10 = 0)
            DB_0
                步骤三：根据range方案，定位哪个Table，(O<id<1000万）
            table_0

        按照上面的流程，我们就可以根据此规则，定位一个id，我们看看有没有避免热点问题。

        我们看一下，id在【0，1000万】范围内的，
        根据上面的流程设计，1000万以内的id都均匀的分配到'DB_0,DB_1,DB_2三个数据库中的Table_0表'中，
        为什么可以均匀，因为我们'用了hash的方案，对10进行取模'。

        上面我们也提了疑问，为什么'对表的总数10取模'，'而不是DB的总数3'进行取模？
        我们看一下为什么DB_0是4张表，其他两个DB_1是3张表？
        在我们安排服务器时，'有些服务器的性能高，存储高'，就'可以安排多存放些数据'，有些'性能低的就少放点数据'。
        
        如果我们取模是按照DB总数3，进行取模，那就代表着【0，4000万】的数据是平均分配到3个DB中的，
        那就不能够实现按照服务器能力适当分配了。

        按照Table总数10就能够达到，看如何达到
        https://pic4.zhimg.com/v2-684243f2caaba153f04c9c0faaec8c63_r.jpg

        上图中我们
        '对10进行取模，如果值为【0，1，2，3】就路由到DB_0'，
        如果值为【4，5，6】路由到DB_1，
        【7，8，9】路由到DB_2。
        
        现在小伙伴们有没有理解，这样的设计就可以把多一点的数据放到DB_0中，
        其他2个DB数据量就可以少一点。DB_0承担了4/10的数据量，DB_1承担了3/10的数据量，
        DB_2也承担了3/10的数据量。
        整个Group01承担了【0，4000万】的数据量。

        注意：小伙伴千万'不要被DB_1或DB_2中table的范围也是0～4000万疑惑'了，
        这个是范围区间，也就是'id在哪些范围内'，'落地到哪个表而已'。
        上面一大段的介绍，就'解决了热点的问题'，以及可以按照服务器指标，设计数据量的分配。

    如何扩容

        其实上面设计思路理解了，'扩容就已经出来了'；那就是'扩容的时候' '再设计一个group02组'，'定义好'此'group的数据范围'就ok了。

        https://pic3.zhimg.com/80/v2-13a34b9590b3ec4b4ad92c971ddaa196_1440w.jpg

        因为是新增的一个group01组，所以'就没有什么数据迁移概念，完全是新增的group组'，
        而且这个group组照样就防止了热点，也就是【4000万，5500万】的数据，
        都均匀分配到三个DB的table_0表中，【5500万～7000万】数据均匀分配到table_1表中。

    系统设计

        思路确定了，设计是比较简单的，就3张表，把group，DB，table之间建立好关联关系就行了。

        'Group表'     
            'group id     group_name     start id      end id'
            1           group01           0           40000000
            2           group02           40000000    80000000

        'DB表'
            'db_id        db_name        group_id        hash_value'
            1            go1_db_0          1               0,1,2.3
            2            g01_db_1          1              4,5,6
            3            g0l db_2            1            7,8,9
            4            g02 db_0            2            0,1,2
            5            g02 db 1            2            3.4.5
            6            g02_db_2            2            6,7,8

        'Table表'
            'table id        table name        db id              start id               end id'
            1               table0               1               0                      10000000
            2               table1               1               10000000               20000000
            3               table2               1               20000000               30000000
            4               table3               1               30000000               40000000

        

        
        上面的表关联其实是比较简单的，只要原理思路理顺了，就ok了。
        小伙伴们在开发的时候'不要每次都去查询三张关联表'，可以保存到缓存中（本地jvm缓存），这样不会影响性能。

        '一旦需要扩容'，小伙伴是不是'要增加一下group02关联关系'，那'应用服务需要重新启动吗'？

        简单点的话，就凌晨配置，重启应用服务就行了。
        但如果是大型公司，是不允许的，因为'凌晨也有订单的'。
        那怎么办呢？本地jvm缓存怎么更新呢？

        其实方案也很多，可以'使用zookeeper'，也'可以使用分布式配置'，这里是'比较推荐使用' '分布式配置中心'的，
        可以将这些'数据配置到分布式配置中心'去。